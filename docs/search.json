[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modeling Choice and RT",
    "section": "",
    "text": "Preface\nThis is a collection of notes and examples that serve as an introduction to cognitive models of choice and response time. To follow along in R, make sure you have the following packages installed.\n\n\nCode\nlibrary(tidyverse)\nlibrary(WienR)\nlibrary(fddm)\nlibrary(rtdists)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Perspective on computational models of cognition\nUnderstanding the mind is ultimately about understanding a dynamic system comprised of a large set of elements that interact in complex ways. If we had designed the mind, things would be easier: Like a car, a computer, or some other piece of modern machinery, we would have built different parts for different functions and stuck them together in a way that would make it easy for us to repair and reconfigure the system. In reality, things are not so simple. Because the mind is not human-engineered (or really “engineered” at all!), for us mere humans to understand it requires us to reverse-engineer the mind. In other words, we have to see what the mind does in some particular situation, build a particular type of machine—a model—that we hypothesize operates like the mind does, and then put the machine in that same scenario and see whether it acts like a human mind. To the extent that our model acts like a human in that scenario, that gives us reason to believe that the model is operating in some sense “like” a human mind does in that scenario.\nIn the early days of scientific psychology, our tools for building models of complex dynamical systems were quite limited. We could build physical models, but these are constrained by the physical properties of the materials we use (nonetheless, it is worth noting that mechanical model of attention by Broadbent (1957) is designed in such a way that the physical properties of the model capture important psychological constraints). Purely mathematical models may not have been so strongly constrained by physicality, but when considering situations with many interacting elements or when stochastic noise is involved, mathematical models quickly become intractable (consider the insoluble “three body problem”—even with a complete model of such a system, we cannot derive its future behavior analytically). As a result, early psychology was dominated by behaviorism, which eschewed the development of theories of the mind and contented itself merely with observing and cataloging behavior.\nIt was not until the middle of the twentieth century, when modern computers began to become of use, that the possibility of “reverse-engineering the mind” became a reality. This was the time of the “cognitive revolution” (Neisser (1967)). The revolution came about for both technical and conceptual reasons.\nFrom a conceptual perspective, computers offered a productive metaphor for helping us understand how the mind works. A computer uses the same physical substrate to perform different functions, similar to how the same brain lets us both speak and play piano. A computer’s adaptivity comes from the fact that the computer can run different “programs” on its hardware. A program is a set of procedures that take a set of “inputs” and transform them into “outputs”. This is analogous to how a living organism decides to act in a certain way (its “outputs”) depending on its goals and on the environment it happens to be in (its “inputs”). Meanwhile, the procedures that transform a computer’s inputs into outputs often involve intermediate steps that do not themselves produce observable behavior but which are nonetheless represented by changes in the internal state of the computer. These internal states are analogous to thoughts or beliefs in that they may not be externally observable, but they are critical steps on the path toward taking an action. The computer metaphor thus enables us to understand cognition in terms of how internal states of mind represent aspects of an organism’s environment, goals, and thoughts in such a way that these representations can be processed to yield behavior that is appropriate to the situation the organism is in.\nFrom a technical perspective, computers offer a way to derive predictions from complex models that would not have been tractable otherwise. As we shall see, this is particularly valuable for two applications: First, we can use the computer to simulate what a model would do and thereby understand the distribution of possible actions it can take. This obviates the need to derive predictions through mathematical analysis or logic, which though powerful, can only be applied to simple models. Second, we can use the computer to fit a model to data. Almost all models have parameters which can be thought of as “knobs” or “settings” that adjust the kind of behavior the model produces. To “fit” a model means to find the parameters for that model that get it to generate behavior that is as close as possible to the behavior recorded in a dataset. Except for very simple models, it is impossible (or at least very impractical) to try to fit them to data without a computer. But as we shall see, fitting a model is useful because we can infer from the best-fitting parameters something about the person who produced the data to which the model was fit.\nIn summary, computers made it feasible for cognitive psychologists to “reverse-engineer” the mind because they (a) provided a valuable conceptual metaphor that allowed theories of cognition to be posed in the form of computational models comprised of internal representations and processes applied to those representations; (b) enabled predictions to be derived for models that were complex and/or had stochastic elements; and (c) enabled those same kinds of models to be “fit” to data so that model parameters can give insight into how a person performed the task for which data was recorded.\nThere is an important difference between “reverse-engineering” a natural system, like the mind, from reverse-engineering a human-designed system like a car. Because a natural system was not “engineered”, the models we devise are not guaranteed to work the same way as a natural system, even if the model accurately mimicks the behavior of the natural system in the cases we study. The purpose in “reverse-engineering” the mind is not to build a duplicate mind, it is instead to “translate” a complex system into a form that enables us to understand it better. The model is a deliberate simplification which we expect to deviate from reality in many ways. What we hope is that we arrive at a model that helps us understand the key features of a natural system well enough for us to understand why it acts the way it does in specific situations (for further discussion of the purposes of models in psychology, see Cox & Shiffrin, In press; Singmann et al., 2022).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#in-this-workshop",
    "href": "intro.html#in-this-workshop",
    "title": "1  Introduction",
    "section": "1.2 In this workshop",
    "text": "1.2 In this workshop\nWith that high-minded philosophical stuff out of the way, the purpose of this workshop is to see how to use computational cognitive models of a particular ilk. These are models that are designed to account for two aspects of behavior: choice and response time (RT). These models are applied in situations where a person (or other organism!) has to decide between a small number of possible alternatives, often just two. Such situations abound in experimental psychology, including lexical decision, recognition memory, detection tasks, search tasks, categorization tasks, etc. The models are designed to help us understand two things:\n\nWhy did a participant make the choices they made?\nWhy did it take the participant a certain amount of time to make the choices they made?\n\n\n1.2.1 Key theoretical constructs\nThe vast majority of models of choice and RT address those two questions by invoking four basic theoretical constructs:\nEvidence accumulation: Choosing from among a set of options is assumed to require accumulating “evidence” that weights either in favor of or against each option. This evidence may come from perception (e.g., deciding which of two stimuli is brighter), from memory (e.g., deciding whether an item was or was not on a previously-studied list), or from deliberation (e.g., deciding which of two products to buy). As such, the idea that decisions are made by accumulating evidence helps explain not only which choice was made (it was the option that was most favored by the accumulated evidence) and how long it took to made the choice (the time needed to accumulate sufficient evidence to commit to a decision).\nResponse caution: If decisions are made by accumulating evidence, there must be a policy that terminates the accumulation process, otherwise someone would keep accumulating evidence forever. This is the idea behind the construct of “response caution”, which is that, depending on the situation, people may adopt a policy of deciding quickly on the basis of very little evidence (low response caution) or deciding slowly by waiting for more evidence to accumulate (high response caution). Thus, this construct is directly related to the idea of speed-accuracy trade-off.\nResponse bias: It may be that a decision maker is willing to commit to some options more readily than others; in that case, we say they are “biased” in favor of some responses. Typically, this bias is modeled by assuming less response caution for some options than others. In other words, a participant may be willing to commit to some decisions on the basis of less accumulated evidence than others.\nResidual time: The time needed to accumulate sufficient evidence to make a decision is not the only thing that contributes to observed response times. After all, it takes time to realize that a trial of a task has actually begun. It may also take time to retrieve relevant information from memory, to focus attention on relevant features in the environment, or to evaluate a potential outcome. Finally, it takes some time to execute the motor actions associated with the chosen option (e.g., to press a button, move a lever, etc.). The time for all of these additional processes is often called non-decision time (NDT) or encoding and response execution time (\\(T_{ER}\\)). However, I prefer to call it simply “residual time” because that is what it is—it is the time “left over” besides the time needed for evidence accumulation.\n\n\n1.2.2 Models and parameters\nThe models we will encounter have parameters that pertain to each of the four constructs just described. We will see how differences in the values of these parameters result in different patterns of behavior from the model. This is also how the models are “fit” to data—by finding the combination of parameter values that produces behavior that most closely resembles that produced by a participant in some task. The value of doing this is that it helps us understand why that participant acted the way they did. For example, we might want to know whether someone was fast because they were able to quickly accumulate the evidence they needed, because they were uncautious, because they were biased, or because they could execute motor actions quickly. We can address that question by finding the values of the parameters associated with each construct that best fit their observed performance.\n\n\n1.2.3 Outline\n\nSimulating a random walk and then a diffusion process, understanding the relationship between model parameters and resulting behavior.\nUsing fddm to estimate diffusion model parameters from data, compare predicted and observed behavior, and do inference via statistics on parameters and via model comparisons (AIC/BIC for groups, wAIC/wBIC for individuals).\n\nIntroduction with simulated data and parameter recovery.\nWork through complete example using “blast” data.\nAdditional example with rr98 data, introducing continuous predictors (strength).\nExercise applying the model to some learning data.\n\nFitting accumulator models (LBA) to data using rtdists and some coding. Show how this can also be used for diffusion models to allow for v\nEBRW: Relating diffusion parameters to cognitive processes.\n\n\n\n\n\nBroadbent, D. E. (1957). A mechanical model for human attention and immediate memory. Psychological Review, 64(3), 205–215.\n\n\nCox, G. E., & Shiffrin, R. M. (In press). Computational models of event memory. In M. J. Kahana & A. Wagner (Eds.), Oxford handbook of human memory. Oxford University Press.\n\n\nNeisser, U. (1967). Cognitive psychology. Appleton-Century-Crofts.\n\n\nSingmann, H., Kellen, D., Cox, G. E., Chandramouli, S. H., Davis-Stober, C. P., Dunn, J. C., Gronau, Q. F., Kalish, M. L., McMullin, S. D., Navarro, D. J., & Shiffrin, R. M. (2022). Statistics in the service of science: Don’t let the tail wag the dog. Computational Brain & Behavior.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "history.html",
    "href": "history.html",
    "title": "2  Historical Perspectives",
    "section": "",
    "text": "2.1 Random walk and diffusion models\nThis chapter provides some historical context for the development of the two main classes of choice-RT models: random walk and diffusion models, which are chiefly employed to model situations where people choose between two options; and counter/accumulator models, which can also model situations with two options but also apply to situations with more than two options.\nEarly models of human choice behavior treated choice as a statistical decision of the sort described by Neyman & Pearson (1933), in which a sample of data (e.g., a stimulus) is presumed to be drawn from a set of underlying generating distributions and the task of the participant is to select the distribution most likely to have generated the data. Specifically, the decision, given data \\(x\\) and hypotheses \\(H_0\\) and \\(H_1\\), is governed by a likelihood ratio:\n\\[\nL(x) = \\frac{\\Pr (H_1 | x)}{\\Pr (H_0 | x)} = \\frac{\\Pr (x | H_1)}{\\Pr (x | H_0)} \\frac{\\Pr (H_1)}{\\Pr (H_0)} \\text{.}\n\\]\nIf this ratio is greater than a threshold value \\(\\theta\\), hypothesis \\(H_1\\) is selected otherwise \\(H_0\\) is chosen. Signal detection theory (Macmillan & Creelman, 2005) is the most prominent theoretical framework in this vein, and continues to be a useful and widely-applied model of choice. Its utility lies in being able to give clear interpretations to notions like bias (e.g., \\(\\theta\\)) and discriminability (proportional to \\(\\frac{\\Pr (x | H_1)}{\\Pr (x | H_0)}\\)). Signal detection theory, however, assumes that the decision is based on a fixed amount of evidence and thus ignores the dynamic process of evidence accumulation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Historical Perspectives</span>"
    ]
  },
  {
    "objectID": "history.html#random-walk-and-diffusion-models",
    "href": "history.html#random-walk-and-diffusion-models",
    "title": "2  Historical Perspectives",
    "section": "",
    "text": "2.1.1 The Sequential Probability Ratio Test\nThe question of how, when evidence accumulates over time, one might arrive at a final decision was first treated by Wald (1945), who proposed a sequential probability ratio test (SPRT). Assume that data samples \\(x_i\\), \\(i = 1, \\dotsc, n\\), arrive one at a time, and are sampled independently from the same underlying generating distribution, which is either \\(H_1\\) or \\(H_0\\). Then, the order in which the samples arrive is immaterial (they are “exchangeable”) and to compute the likelihood ratio \\(L(n)\\) given the \\(n\\) samples we currently possess, we need only multiply the ratios for each sample:\n\\[\nL(n) = \\left[ \\prod_{i=1}^n \\frac{\\Pr (x_i | H_1)}{\\Pr (x_i | H_0)} \\right] \\frac{\\Pr (H_1)}{\\Pr (H_0)} \\text{.}\n\\]\nThus, taking the logarithm of the likelihood ratio, we can express the change in log-likelihood that results from the \\(t\\)th new sample:\n\\[\n\\Delta L(t) = \\log \\frac{\\Pr (x_t | H_1)}{\\Pr (x_t | H_0)} \\text{,}\n\\]\nwhere \\(L(0) = \\log \\frac{\\Pr (H_1)}{\\Pr (H_0)}\\), the log of the ratio of the priors. The question that Wald addressed in this context was when to stop collecting new data samples and make a decision. Define two thresholds, \\(\\theta_1\\) and \\(\\theta_0\\), such that the decision maker will decide \\(H_1\\) if \\(L(t) \\geq \\theta_1\\) and will decide \\(H_0\\) if \\(L(t) \\leq \\theta_0\\) and \\(\\theta_1 &gt; 0 &gt; \\theta_0\\). Assuming the decision maker has a desired type I error rate \\(\\alpha\\) (probability of deciding in favor of \\(H_1\\) when \\(H_0\\) was the true generating distribution) and type II error rate \\(\\beta\\) (probability of deciding \\(H_0\\) when \\(H_1\\) is true), Wald (1945) showed that the thresholds should be set such that \\(\\theta_1 \\leq \\log (1 - \\beta) - \\log \\alpha\\) and \\(\\theta_0 \\geq \\log \\beta - \\log (1 - \\alpha)\\) in order to achieve maximum power. The thresholds may not be exactly equal to the specified values as a consequence of the fact that evidence arrives in a discrete manner and hence might “jump” over a threshold rather than actually equaling it.\nAn important consequence of the SPRT is that the amount of evidence required to make a decision for either \\(H_1\\) or \\(H_0\\)—and thus, in general, the number of samples required to make a decision—is tightly coupled to the expected error rates. For example, if the decision maker is willing to make more type I errors (with rate \\(\\alpha\\)), then both \\(\\theta_1\\) and \\(\\theta_0\\) will decrease, reflecting an increased willingness to respond \\(H_1\\) and not \\(H_0\\) (assuming \\(\\beta\\) is held constant). This has the consequence that, on average, it will take less time (fewer samples) before the decision maker is willing to respond \\(H_1\\) and more time (more samples) before the decision maker is willing to commit to \\(H_0\\). Thus, speed and accuracy can trade-off with one another, and the speed of particular responses (whether correct or incorrect) can be affected by the biases of the decision maker.\nStone (1960) suggested that the psychological process of decision making might be governed by something akin to the SPRT, an idea that received more explicit Bayesian treatment in Edwards (1965). Unfortunately, the SPRT predicts that the number of samples to reach a decision is independent of whether that decision was correct or not. That is, error and correct decision times are predicted to be equal (or, at least, drawn from the same distribution). This is very rarely true in observed data, hence a more general decision model is called for.\n\n\n2.1.2 Random Walk\nThe sequential probability ratio test can be considered a kind of stationary random walk in discrete time and continuous space. This is a consequence of the fact that evidence arrives sequentially and perturbs the decision maker’s current beliefs (\\(L(t)\\)) to a degree that is independent of those beliefs. Since the evidence acquired on each time step, \\(x_t\\), is sampled independently, we can treat the resulting likelihood ratio change \\(\\Delta L(t)\\) as an independent sample from some distribution. We can thus consider \\(L(t)\\) to simply accumulate samples of a random variable with density \\(f(\\Delta L)\\) and moment generating function \\(M(\\phi)\\).\nThe evidence density will, of course, depend on the true state of the world, such that if \\(H_1\\) is true, the evidence density \\(f_1(\\Delta L)\\) will produce more evidence in favor of \\(H_1\\) and likewise for density \\(f_0(\\Delta L)\\). The properties of the moment generating functions of these densities, \\(M_1(\\phi)\\) and \\(M_0(\\phi)\\), respectively, determine the properties of the random walk’s predicted RT’s. Because, in the SPRT, evidence takes the form of a likelihood ratio, the evidence distributions are symmetric, i.e., \\(f_1(\\Delta L) = f_0(-\\Delta L)\\), meaning the moment generating functions are related by translation: \\(M_1(\\phi + 1) = M_0(\\phi)\\). Swensson & Green (1977) showed that any random walk model for which the moment generating functions of the evidence distributions are related by translation must predict equal RTs to error and correct responses.\nA random walk model of human response dynamics was studied extensively as part of the theory of relative judgment (Link, 1975; Link & Heath, 1975). These authors generally assumed that the distributions of differences \\(\\Delta L(t)\\) are symmetric with respect to the two alternatives, in the way described above. They further assumed that the decision thresholds were symmetric, i.e., \\(\\theta_1 = -\\theta_0\\). Despite these restrictions, this random walk model can predict different RT’s for errors and correct responses. In particular, the difference between mean correct and error RTs in the theory of relative judgment is given by \\[\n\\begin{align}\nRT( \\text{Respond } H_1 | H_1) - RT( \\text{Respond } H_0 | H_1) & = \\left( \\frac{\\theta_1}{\\mu_1} \\right) \\left( \\frac{\\gamma_1 - 1}{\\gamma_1} \\right) \\\\\nRT( \\text{Respond } H_0 | H_0) - RT( \\text{Respond } H_1 | H_0) & = \\left( \\frac{\\theta_0}{\\mu_1} \\right) \\left( \\frac{\\gamma_1 - 1}{\\gamma_1} \\right)\n\\end{align}\n\\]\nwhere \\(\\mu_i\\) is the mean of evidence density \\(f_i(\\Delta L)\\) and \\(\\gamma_i\\) is its third moment, and \\(\\mu_1 = -\\mu_0\\) and \\(\\gamma_1 = -\\gamma_0\\) (Link & Heath, 1975). Thus, if \\(\\gamma_1 \\neq 1\\), errors may be either faster than correct responses (\\(\\gamma_1 &gt; 1\\)) or slower (\\(\\gamma_1 &lt; 1\\)). Critically, however, the relationship between error and correct RT must be of the same sign regardless of the true evidence generating distribution, which need not be the case in observed data. By relaxing the assumptions of symmetry, such that the moment generating functions of the evidence distributions are related by both scaling and translation (\\(M_0(\\phi) = k M_1(\\phi + 1)\\), where \\(k\\) is a constant), Ashby (1983) was able to show that a random walk could predict any ordering of mean RTs for correct and error responses.\n\n\n2.1.3 The Wiener Diffusion Process\nThe evidence generating density \\(f_i(\\Delta L)\\) can be decomposed into an expectation, \\(\\mu\\), and a noise distribution \\(\\nu\\), which is simply the original distribution with the expected value (\\(\\mu\\)) subtracted. The resulting stochastic difference equation for \\(L(t)\\) is\n\\[\n\\Delta L(t) = \\left[ \\mu + \\nu(t) \\right] \\Delta t \\text{,}\n\\]\nwhere we have now explicitly introduced \\(\\Delta t\\) (up to now, \\(\\Delta t = 1\\)) and \\(\\nu(t)\\) represents a sample drawn from the noise distribution \\(\\nu\\) at time \\(t\\).\nLet us now assume that the evidence generating distribution is Gaussian with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then, we can consider the noise distribution in the above difference equation to be a Gaussian distribution with mean zero (we have subtracted out \\(\\mu\\)) and variance \\(\\sigma^2\\), which we denote \\(\\mathcal{N}(0, \\sigma^2)\\). We now have\n\\[\n\\Delta L(t) = \\left[ \\mu + \\mathcal{N}(0, \\sigma^2) \\right] \\Delta t \\text{.}\n\\]\nIn addition, we can rewrite \\(\\mathcal{N}(0, \\sigma^2)\\) as a sample from a standard (zero mean, unit variance) Gaussian, multiplied by \\(\\sigma\\). This gives us\n\\[\n\\Delta L(t) = \\mu \\Delta t + \\mathcal{N}(0, 1) \\sigma \\sqrt{\\Delta t} \\text{.}\n\\]\nBy taking the limit \\(\\Delta t \\rightarrow 0\\), we can move from discrete time to continuous time. In so doing, we have transformed the Gaussian random walk into the Wiener drift-diffusion process:\n\\[\nd L(t) = \\mu d t + \\mathcal{N}(0, 1) \\sigma \\sqrt{dt} \\text{.}\n\\]\nRatcliff (1978) modeled human responses as the outcome of a Wiener diffusion where the mean drift rate, \\(\\mu\\), reflects the relative amount of evidence favoring one response over the other while \\(\\sigma\\) reflects the noise inherent in the evidence-sampling process. Because each infinitesimal step of the evidence accumulation process comes from a normal distribution, the density over the accumulated evidence at time \\(t\\) is another normal distribution:\n\\[\n\\Pr (L(t) = x | t) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2 t}} \\exp \\left[ \\frac{\\left(x - \\mu t \\right)^2}{2 \\sigma^2 t} \\right] \\sim \\mathcal{N}(\\mu t, \\sigma^2 t) \\text{.}\n\\]\nThis makes it easy to calculate \\(d'\\), the familiar index of sensitivity from SDT, in the context of the diffusion model. Ratcliff (1978) assumed that the mean drift rate \\(\\mu\\) was itself sampled from a normal distribution with mean \\(u\\) and variance \\(\\eta\\) for each stimulus. We must thus integrate over \\(\\mu\\) to express the distribution of evidence as a function of time:\n\\[\n\\begin{align}\n\\Pr (L(t) = x | t) = & \\int_{-\\infty}^{\\infty} \\mathcal{N}(\\mu t, \\sigma^2 t) \\mathcal{N}(u, \\eta) d \\mu \\\\\n= & \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2 \\pi \\sigma^2 t}} \\exp \\left[ \\frac{\\left(x - \\mu t \\right)^2}{2 \\sigma^2 t} \\right] \\frac{1}{\\sqrt{2 \\pi \\eta^2}} \\exp \\left[ \\frac{\\left(\\mu - u \\right)^2}{2 \\eta^2} \\right] d \\mu \\\\\n= & \\frac{1}{\\sqrt{2 \\pi t ( \\eta^2 t + \\sigma^2 )}} \\exp \\left[ \\frac{\\left(x - u t \\right)^2}{2 t ( \\eta^2 t + \\sigma^2) } \\right] \\sim \\mathcal{N}(u t, t ( \\eta^2 t + \\sigma^2)) \\text{.}\n\\end{align}\n\\]\nIf \\(u\\) is the mean of the signal distribution (from which mean drift rates are drawn) and \\(v\\) is the mean of the noise distribution, \\(d'(t)\\) as a function of time \\(t\\) is simply:\n\\[\nd'(t) = \\frac{ut - vt}{ \\sqrt{t (\\eta^2 t + \\sigma^2)} } = \\frac{u - v}{\\eta \\sqrt{1 + \\frac{\\sigma^2}{\\eta^2 t}}} \\text{.}\n\\]\nThis is a negatively-accelerated function of time, and reaches an asymptote as \\(t \\rightarrow \\infty\\), \\(d'(t) \\rightarrow \\frac{u - v}{\\eta}\\). This formula for \\(d'\\) will prove useful in subsequent applications of the diffusion model to situations with nonstationary evidence distributions.\nFinally, we note the diffusion model predictions for response probabilities and latencies: Assuming that the process starts at some point \\(z &gt; 0\\) and \\(\\theta_1 &gt; z\\) and \\(\\theta_0 = 0\\), the probability of reaching \\(\\theta_0 = 0\\) (making a \\(H_0\\) decision) is\n\\[\n\\Pr ( H_0 | \\mu, z, \\theta_1, \\sigma ) = \\frac{ \\exp \\left( - \\frac{2 \\mu \\theta_1}{\\sigma^2} \\right) - \\exp \\left( - \\frac{2 \\mu z}{\\sigma^2} \\right)}{\\exp \\left( - \\frac{2 \\mu \\theta_1}{\\sigma^2} \\right) - 1}\n\\]\nwhile the probability density for times \\(t\\) at which \\(\\theta_0\\) is reached is\n\\[\nf_0 (t | \\mu, z, \\theta_1, \\sigma ) = \\frac{1}{\\Pr (H_0 | \\mu, z, \\theta_1, \\sigma)} \\frac{\\pi \\sigma^2}{\\theta_1^2} \\exp \\left( - \\frac{z \\mu}{\\sigma^2} \\right) \\sum_{k=1}^{\\infty} k \\sin \\left( \\frac{\\pi z k}{\\theta_1} \\right) \\exp \\left[- \\frac{1}{2} t \\left( \\frac{\\mu^2}{\\sigma^2} + \\frac{\\pi^2 k^2 \\sigma^2}{\\theta_1^2} \\right) \\right] \\text{.}\n\\]\nThe probability of making a \\(H_1\\) response is simply \\(1 - \\Pr (H_0 | \\mu, z, \\theta_1, \\sigma )\\) while the density for response times given an \\(H_1\\) response is\n\\[\n\\begin{align}\nf_1 (t | & \\mu, z, \\theta_1, \\sigma ) = \\\\\n& \\frac{1}{\\Pr (H_1 | \\mu, z, \\theta_1, \\sigma)} \\frac{\\pi \\sigma^2}{\\theta_1^2} \\exp \\left( \\frac{(\\theta_1 - z) \\mu}{\\sigma^2} \\right) \\sum_{k=1}^{\\infty} k \\sin \\left( \\frac{\\pi (\\theta_1 - z) k}{\\theta_1} \\right) \\exp \\left[ \\frac{1}{2} t \\left( \\frac{\\mu^2}{\\sigma^2} - \\frac{\\pi^2 k^2 \\sigma^2}{\\theta_1^2} \\right) \\right] \\text{.}\n\\end{align}\n\\]\nThese reaction time distributions are positively-skewed, as is usually observed in reaction time distributions. More “difficult” decisions, in which \\(u\\) and \\(v\\) do not differ substantially, thus reducing the absolute value of \\(\\mu\\), will naturally result not only in longer reaction times, but more skewed reaction times according to the diffusion model.\n\n\n2.1.4 Optimality and Multiple Alternatives\nThe Wiener diffusion process, as a natural extension of the SPRT, can be shown to be the optimal decision strategy when there are two alternatives and the evidence-generating distribution is stationary and perturbed by Gaussian noise (Bogacz et al., 2006). The diffusion model can be extended to model choices between more than two alternatives by extending the Wiener process into multiple dimensions (Smith, 2000). If the decision thresholds are orthogonal to one another (which might occur if they are parallel to the coordinate axes), a multidimensional diffusion essentially becomes a parallel race model (see below). While this extension is not conceptually difficult, it runs into the problem that a diffusion in more than two dimensions is no longer guaranteed to eventually hit every point in the state space. Thus, it is far harder to calculate the probability of a multidimensional diffusion process hitting a boundary, let alone the density of its first passage times. There are cases, however, where a multidimensional diffusion may be reduced a more tractable one in a single dimension (Ashby, 2000; Smith, 2000).\nInstead of treating a decision between multiple alternatives as a multidimensional diffusion, it can be explicitly modeled as a set of simultaneous diffusions that independently accumulate evidence for each alternative. Just as the unidimensional diffusion is equivalent to the SPRT, a set of \\(M\\) parallel diffusions is related to the “\\(M\\)-ary Sequential Probability Ratio Test” [MSPRT; Baum & Veeravalli (1994)]. The MSPRT has no single optimal decision rule, but one candidate procedure is to choose option \\(H_k\\) when its posterior probability given the evidence \\(X\\) up to time \\(t\\), \\(\\Pr (H_k | X_t)\\), exceeds a threshold level of probability. This threshold is equal to the expected hit rate, conditioned on response \\(H_k\\). This rule is optimal to the extent that it produces a decision with the desired level of accuracy in the shortest time (Bogacz & Gurney, 2007; Jones et al., 2009; Zhang & Chang, in review).\nTo formally express the diffusion version of the MSPRT, recall that Bayes rule relates the posterior probability of a hypothesis \\(H_k\\) to the likelihood, prior, and evidence: \\[\n\\Pr (H_k | X_t) = \\frac{\\Pr (X_t | H_k) \\Pr (H_k)}{\\sum_{i=1}^M \\Pr (X_t | H_i) \\Pr (H_i)} \\text{.}\n\\] Taking the logarithm of the above yields \\[\n\\log \\Pr (H_k | X_t) = \\log \\Pr (X_t | H_k) + \\log \\Pr (H_k) - \\log \\left[ \\sum_{i=1}^M \\exp \\left( \\log \\Pr (X_t | H_i) + \\log \\Pr (H_i) \\right) \\right] \\text{.}\n\\] Assuming, as we have all along, that evidence arrives sequentially and is sampled independently from a stationary evidence-generating distribution, we can let \\(L_k(t) = \\sum_{i=0}^t \\log \\Pr (x_i | H_k) + \\log \\Pr (H_k)\\), such that \\(\\Delta L_k(t) = \\log \\Pr (x_t | H_k) \\Delta t\\). Finally, by assuming that the likelihood functions are Gaussian and that \\(\\Delta t \\rightarrow 0\\), we obtain a Wiener drift-diffusion process for each alternative. The decision rule for the multi-alternative diffusion model thus requires that the aggregate state for an option \\(k\\) be sufficiently large relative to the sum of the aggregate states of all alternatives. Thus, while evidence may accumulate for each alternative independently and in parallel, the alternatives compete with each other in order to actually produce a decision. In the two-alternative case, the predictions are identical to the standard SPRT (and, thus, to the unidimensional diffusion model), in which only two alternatives compete with one another. This extension of the diffusion model to multiple alternatives can be made even more powerful by assuming that the decision maker must infer the underlying mean drift rates for each option, leading to a rational explanation for certain trial-order effects (Jones et al., 2009).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Historical Perspectives</span>"
    ]
  },
  {
    "objectID": "history.html#counter-and-accumulator-models",
    "href": "history.html#counter-and-accumulator-models",
    "title": "2  Historical Perspectives",
    "section": "2.2 Counter and Accumulator Models",
    "text": "2.2 Counter and Accumulator Models\nThe extension of the diffusion model to multiple alternatives leads directly into a potentially more general class of models for response dynamics, namely, the counter or accumulator models. The two-alternative SPRT, random walk, and diffusion models necessarily consider evidence for one alternative to be equal evidence the other alternative. This is also the case when there are more than two alternatives. However, there are situations in which it does not make sense to consider strict competition among alternatives. Consider a situation in which you are choosing between two desserts of equal price, the key lime pie and the German chocolate cake. The fact that you like coconut might increment your preference for German chocolate cake, but it is hard to see why it should reduce your preference for key lime pie. You may have different degrees of preference for either option, and that difference may drive your eventual decision, but there is no reason to think your preferences should be anti-correlated, as is assumed by the models we have considered thus far.\n\n2.2.1 Counter Models\n“Counter” models are so called because, in their original form, each choice alternative was presumed to be associated with a count of the number of “pieces” of evidence in favor of it, and the corresponding response would be made when one counter reached a threshold. The first such model was by LaBerge (1962), in which time progressed in discrete intervals of equal size. On each time-step, a single “piece” of evidence was added to a counter representing one of the possible alternatives, and a final decision made when one of the counters reaches a threshold. In other words, each alternative “races” to reach its threshold and produce a response.\nTwo (not mutually exclusive) ways to extend the counter model are apparent: First, to make the state space (the possible evidence values) continuous, rather than discrete; and second, to make time continuous rather than discrete. The accumulator model of Vickers (1970) and Smith & Vickers (1988) assumes that time remains discrete while the values of evidence may take on continuous values, perhaps from a truncated normal or exponential distribution. The parameters of each distribution reflect the amount of evidence for each alternative, and the thresholds on each accumulator reflect response biases. Alternatively, the Poisson counter model (Pike, 1973; Smith & Van Zandt, 2000; Townsend & Ashby, 1983) assumes that the amount of evidence accumulated at any one time is fixed, but the interarrival times for that evidence are variable. Specifically, they are sampled independently from an exponential distribution, with rates that differ between accumulators. In the Poisson counter model, thresholds again reflect response biases, while the rate of arrivals to a particular counter corresponds to the amount of evidence in favor of that response. Thus, unless there is a bias against the correct response, the Poisson counter model will predict faster RT for correct responses than errors, no matter what sources of variability are added to the model (Ratcliff & Smith, 2004).\nWhen \\(M\\) Poisson counters are running independently and in parallel with interarrival rates \\(\\lambda_1, \\lambda_2, \\dotsc, \\lambda_M\\), the entire decision process can be construed as a Poisson process with a rate that is the sum of the rates of the individual accumulators, \\(\\lambda_1 + \\lambda_2 + \\dotsb + \\lambda_M\\) (Townsend & Ashby, 1983). Thus, regardless of the final choice, if it is reached after \\(N\\) counts have accumulated across all the counters, the RT distribution is a Gamma distribution: \\[\nf (t | N, \\lambda_1, \\lambda_2, \\dotsc, \\lambda_M) = \\left(\\sum_{i=1}^M \\lambda_i \\right)^{-N} \\exp \\left( -\\frac{t}{\\sum_{i=1}^M \\lambda_i} \\right) \\frac{t^{N-1}}{\\left(N - 1 \\right)!} \\text{.}\n\\] This has two important consequences: First, RT distributions are predicted to become less skewed as the mean number of samples \\(N\\) increases (the skewness of a Gamma distribution is \\(\\frac{2}{\\sqrt{N}}\\)), which is not supported by most empirical RT distributions. More importantly, however, RT is predicted to be faster with increasing number of alternatives, owing to the fact that each counter contributes \\(\\lambda_i\\) to the overall processing rate. This is a general property of models with independent parallel accumulators for each decision, regardless of the choice of distributions or parameters (Townsend & Ashby, 1983). There are many situations, however, in which increasing the number of alternative responses (and thus the number of parallel accumulators) results in an increase in reaction time. This can limit the utility of parallel counter models to account for observed response dynamics unless accumulation rates decrease with the number of accumulators (Ratcliff & Smith, 2004; Teodorescu & Usher, 2013).\n\n\n2.2.2 Partially Correlated Accumulators\nIf random walk and diffusion models involve accumulators for each alternative that are perfectly anticorrelated, and counter/accumulator models posit accumulators that are totally uncorrelated, perhaps a middle ground might be found? The Leaky, Competing Accumulator model (LCA, Usher & McClelland, 2001) is one such example. In this model, each of the \\(M\\) accumulators is presumed to be subject to Gaussian noise and can be described by the following differential equation: \\[\nd L_i(t) = \\left( I_i - \\gamma L_i(t) - \\beta \\sum_{j \\neq i}^M L_j(t) \\right) dt + \\sigma \\mathcal{N}(0, 1) \\sqrt{dt} \\text{.}\n\\] This equation has several terms that we now parcel out: \\(I_i\\) is the exogenous (stimulus-driven) input to accumulator \\(i\\), analogous to the drift rate in a diffusion; the \\(-\\gamma L_i(t)\\) term implements “leakage”, similar to an OU process (see below) when \\(0 \\leq \\gamma \\leq 1\\); \\(\\beta \\sum_{j \\neq i}^M L_j(t)\\) implements a (partial) negative correlation between accumulators; and the final term is simply the familiar Gaussian noise term. There is, further, a critical nonlinearity in the model, namely, that \\(L_i\\) is never permitted to be negative. The theoretical consequence of this is that accumulators can only act to inhibit one another. The practical consequence is that the LCA model is not analytically tractable, although it can be approximated by two OU processes in the two-accumulator case (Bogacz et al., 2007; Usher & McClelland, 2001). This nonlinearity may, however, prove advantageous by reducing excessive inhibition from poor choices (Bogacz et al., 2007).\nIf this nonlinearity is ignored, the LCA can be made identical, at least in the two-alternative case, to a race model (by letting \\(\\gamma = \\beta = 0\\)), to a diffusion model (by letting \\(\\gamma = 0\\) and \\(\\beta = 1\\)), or to an OU process (by letting \\(\\beta = 1\\)). Thus, it has great flexibility in predicting a variety of patterns of reaction time and accuracy, especially when incorporating payoffs (Usher & McClelland, 2004).\n\n\n2.2.3 Ballistic Accumulators\nAlthough they will not be addressed subsequently, we briefly alight on a recent set of models that are “ballistic” in the sense that they are completely deterministic, once their initial conditions (parameters, starting points, and thresholds) are set. The first of these was proposed by Brown & Heathcote (2005) as a non-stochastic version of the LCA model. It is nonetheless able to account for decision data because, although the within-trial dynamics of the model are deterministic, it is subject to between-trial noise. In particular, the exact external input to each accumulator is perturbed by Gaussian noise on each trial, and the initial value of each accumulator is randomly sampled from a uniform distribution. Brown & Heathcote (2008) extend this idea to the diffusion model in the Linear Ballistic Accumulator (LBA). This, too, is able to account for observed reaction time and accuracy data quite well (Donkin et al., 2011).\nSequential sampling models rely on the idea that at any one time, evidence may be noisy, and so to make accurate decisions, noisy evidence samples must be integrated over time. What, then, are we to make of the idea that ballistic models without noisy evidence can still fit data? I argue that it is a question of interpretation: Sequential sampling models, from the SPRT to the diffusion model, attempt to model what is thought to be the “real” state of the world: that evidence itself is noisy. Ballistic models, on the other hand, are best thought of as “measurement models”, in that they re-describe or summarize the data (accuracy and reaction time) in a way that makes it amenable to cognitive interpretation. The fact that, by abandoning the notion of sampling noise, ballistic models are still able to fit data should not worry anyone: Ballistic models are trying to describe the data by making as few assumptions as possible, while stochastic models are trying instead to describe the underlying state of the world.\n\n\n\n\nAshby, F. G. (1983). A biased random walk model for two choice reaction times. Journal of Mathematical Psychology, 27, 277–297.\n\n\nAshby, F. G. (2000). A stochastic version of general recognition theory. Journal of Mathematical Psychology, 44, 310–329.\n\n\nBaum, C. W., & Veeravalli, V. V. (1994). A sequential procedure for multihypothesis testing. IEEE Transactions on Information Theory, 40(6), 1994–2007.\n\n\nBogacz, R., Brown, E., Moehlis, J., Holmes, P., & Cohen, J. D. (2006). The physics of optimal decision making: A formal analysis of models of performance in two-alternative forced-choice tasks. Psychological Review, 113(4), 700–765.\n\n\nBogacz, R., & Gurney, K. (2007). The basal ganglia and cortex implement optimal decision making between alternative actions. Neural Computation, 19, 442–477.\n\n\nBogacz, R., Usher, M., Zhang, J., & McClelland, J. L. (2007). Extending a biologically inspired model of choice: Multi-alternatives, nonlinearity and value-based multidimensional choice. Philosophical Transactions of the Royal Society B, 362, 1655–1670.\n\n\nBrown, S., & Heathcote, A. (2005). A ballistic model of choice response time. Psychological Review, 112(1), 117–128.\n\n\nBrown, S., & Heathcote, A. (2008). The simplest complete model of choice response time: Linear ballistic accumulation. Cognitive Psychology, 57, 153–178.\n\n\nDonkin, C., Brown, S., Heathcote, A., & Wagenmakers, E.-J. (2011). Diffusion versus linear ballistic accumulation: Different models but the same conclusions about psychological processes? Psychonomic Bulletin & Review, 55, 140–151.\n\n\nEdwards, W. (1965). Optimal strategies for seeking information: Models for statistics, choice reaction times, and human information processing. Journal of Mathematical Psychology, 2, 312–329.\n\n\nJones, M., Mozer, M., & Kinoshita, S. (2009). Optimal response initiation: Why recent experience matters. In D. Koller, D. Schuurmans, Y. Bengio, & L. Bottou (Eds.), Advances in neural information processing systems (Vol. 21, pp. 785–792).\n\n\nLaBerge, D. (1962). A recruitment theory of simple behavior. Psychometrika, 27(4), 375–396.\n\n\nLink, S. W. (1975). The relative judgment theory of two choice response time. Journal of Mathematical Psychology, 12, 114–135.\n\n\nLink, S. W., & Heath, R. A. (1975). A sequential theory of psychological discrimination. Psychometrika, 40, 77–105.\n\n\nMacmillan, N. A., & Creelman, C. D. (2005). Detection theory: A user’s guide (2nd ed.). Erlbaum.\n\n\nNeyman, J., & Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231, 289–337.\n\n\nPike, R. (1973). Response latency models for signal detection. Psychological Review, 80(1), 53–68.\n\n\nRatcliff, R. (1978). A theory of memory retrieval. Psychological Review, 85(2), 59–108.\n\n\nRatcliff, R., & Smith, P. L. (2004). A comparison of sequential sampling models for two-choice reaction time. Psychological Review, 111(2), 333–367.\n\n\nSmith, P. L. (2000). Stochastic dynamic models of response time and accuracy: A foundational primer. Journal of Mathematical Psychology, 44(3), 408–463.\n\n\nSmith, P. L., & Van Zandt, T. (2000). Time-dependent Poisson counter models of response latency in simple judgment. British Journal of Mathematical and Statistical Psychology, 53, 293–315.\n\n\nSmith, P. L., & Vickers, D. (1988). The accumulator model of two-choice decision. Journal of Mathematical Psychology, 32, 135–168.\n\n\nStone, M. (1960). Models for choice-reaction time. Psychometrika, 25, 251–260.\n\n\nSwensson, R. G., & Green, D. M. (1977). On the relations between random walk models for two-choice response times. Journal of Mathematical Psychology, 15, 282–291.\n\n\nTeodorescu, A. R., & Usher, M. (2013). Disentangling decision models: From independence to competition. Psychological Review, 120(1), 1–38.\n\n\nTownsend, J. T., & Ashby, F. G. (1983). Stochastic modeling of elementary psychological processes. Cambridge University Press.\n\n\nUsher, M., & McClelland, J. L. (2001). The time course of perceptual choice: The leaky, competing accumulator model. Psychological Review, 108(3), 550–592.\n\n\nUsher, M., & McClelland, J. L. (2004). Loss aversion and inhibition in dynamical models of multialternative choice. Psychological Review, 111(3), 757–769.\n\n\nVickers, D. (1970). Evidence for an accumulator model of psychophysical discrimination. Ergonomics, 13(1), 37–58.\n\n\nWald, A. (1945). Sequential tests of statistical hypotheses. The Annals of Mathematical Statistics, 16(2), 117–186.\n\n\nZhang, J., & Chang, M. (in review). A Bayesian random-walk model of choice reaction times under prior knowledge of target onset uncertainty. Proceedings of the National Academy of Sciences.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Historical Perspectives</span>"
    ]
  },
  {
    "objectID": "random_walk.html",
    "href": "random_walk.html",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "",
    "text": "3.1 Out for a random walk\nYou are in a bind. There are two piles of hay in front of you. They look pretty similar in size. You are hungry. Which of the two piles do you walk toward? I should also mention that you are a donkey, so this is pretty important to you! (The deliberating donkey shown above, Simon, is the resident donkey at Indian Ladder Farms near Albany, NY.)\nThe situation described above is a version of the parable of Buridan’s ass. In the parable, the unfortunate ass is unable to come to a decision because the two piles of hay are equally large, meaning he has no basis for choosing between them. As a result, the donkey starves to death.\nIn this chapter, we will build a model of two-choice behavior that rescues Buridan’s donkey from this dire fate. This model is called a random walk. As we will see, this model can be applied to any situation involving a choice between two options. The random walk model instantiates all four of the key constructs described in the Introduction. It is also a great example of the general structure of a computational cognitive model, in that it describes how an observable action (a choice) arises by applying a process of accumulation to a representation of the balance of evidence between the two options.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#out-for-a-random-walk",
    "href": "random_walk.html#out-for-a-random-walk",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "",
    "text": "3.1.1 Representing the current state of evidence\nThe random walk model assumes that, at any given time, a decision maker represents the current balance of evidence between two options as a number. We will creatively refer to this representation as \\(x(t)\\), where \\(x\\) stands for evidence and \\((t)\\) stands for the fact that it is the evidence at a specific time \\(t\\). The sign and magnitude of \\(x(t)\\) represents the extent to which the current value of evidence favors one option over the other.\nIf \\(x(t)\\) equals zero, then the evidence at time \\(t\\) does not favor either option. This is akin to the situation when Buridan’s ass first encounters the two piles of hay. If \\(x(t) &gt; 0\\), then the evidence favors one of the two options. For Buridan’s ass, perhaps positive values of evidence represent evidence in favor of going toward the pile of hay on the right. If \\(x(t) &lt; 0\\), then the evidence favors the other option. For Buridan’s ass, maybe negative values of evidence represent evidence in favor of going toward the pile of hay on the left. Notice that we could just as easily do it the other way around: positive evidence favors going left while negative evidence favors going right. The important thing is just that the two options are associated with opposite signs of evidence.\nIn a cognitive task, the two choices might be “word” and “non-word” in a lexical decision task, “old” and “new” in a recognition memory task, “present” and “absent” in a visual search task, “same” and “different” in a change detection task, “category A” and “category B” in a categorization task, etc. Again, the point is that, at any given time, the degree to which the decision maker’s accumulated evidence at time \\(t\\) favors one option or the other is represented by the value of a number \\(x(t)\\), with each option associated with opposite signs.\n\n\n3.1.2 Accumulating evidence\nThe value of \\(x(t)\\) represents the evidence that has been accumulated by time \\(t\\). But what does it mean to “accumulate” evidence? And what is the “evidence” that is accumulated?\nIn a random walk model, we assume that at regular time intervals (each interval has duration \\(Delta t\\)), the decision maker receives a “sample” of evidence, which we will label \\(\\Delta x(t)\\). This sample can take one of two values, \\(+1\\) or \\(-1\\). If it is \\(+1\\), the sample favors the option associated with positive evidence values (e.g., the pile of hay on the right) and if it is \\(-1\\), the sample favors the option associated with negative evidence values (e.g., the pile of hay on the left). To accumulate evidence means to add the new sample \\(\\Delta x(t)\\) to the current value of the accumulated evidence, i.e.: \\[\n\\overbrace{x(t + \\Delta t)}^{\\text{Updated evidence}} = \\overbrace{x(t)}^{\\text{Current accumulated evidence}} + \\overbrace{\\Delta x(t)}^{\\text{Current sample of evidence}}\n\\] Thus, the accumulated evidence \\(x(t)\\) is the sum of all the samples of evidence that were obtained by time \\(t\\).\n\n\n3.1.3 What is evidence?\nAt this point, it would be reasonable to ask where these samples of evidence come from. There is no single answer to this question because the random walk model, like most of the models of choice and RT we will consider, treats evidence in a very abstract sense. To return to Buridan’s ass, the evidence might be perceptual in nature: For an interval of time, the donkey looks at both piles of hay. Even though both piles are, by assumption, equally big, that may not always be visually apparent. During any finite interval of time, one pile might happen to look ever so slightly larger than the other, perhaps due to a quirk of the light, a sheaf fluttering in the breeze, the donkey’s visual acuity, etc. If the pile on the right happened to look a bit bigger than the one on the left during one of those intervals, then the sample of evidence for that interval would be \\(+1\\). Otherwise, it would be \\(-1\\). Because these minute differences are due to essentially chance factors, and they are equally likely to favor either pile, we can say that the probability of getting a sample that is either \\(+1\\) or \\(-1\\) is \\(0.5\\). While the evidence might not favor one pile over the other in the long run, it will favor one option over a finite interval of time, which is all any real decision maker has at their disposal. As we shall see shortly, this is the key to saving Buridan’s ass.\nTreating evidence as due, at least in part, to chance factors is why this model is called a “random” walk. It also highlights the fact that the evidence samples need not occur with equal frequency. Perhaps samples come up \\(+1\\) with probability \\(p\\) and otherwise come up \\(-1\\), like the proverbial biased coin flip. If the evidence consistently favors one option, that means that \\(p\\) is close to either 1 or 0. To the extent that chance factors influence the evidence, \\(p\\) will be closer to \\(0.5\\). We have now been introduced to the first parameter of the random walk model: \\(p\\), the probability of getting a sample of evidence that favors the option associated with positive evidence.\nThe figure below illustrates different ways that evidence might accumulate over time. Each step up or down is driven by the sample of evidence that was obtained at that time, which is assumed to be random with probability \\(p\\). The figure also illustrates why this model is called a random “walk”, because each trajectory kind of looks like a path that someone might have walked.\n\n\nCode\nexpand_grid(p = c(0.2, 0.5, 0.8), sim_index = 1:5, t = 1:20) %&gt;%\n    mutate(x_sample = 2 * rbinom(n = n(), size = 1, prob = p) - 1) %&gt;%\n    group_by(p, sim_index) %&gt;%\n    mutate(x_accum = cumsum(x_sample)) %&gt;%\n    ggplot(aes(x = t, y = x_accum, color = factor(p), group = interaction(p, sim_index))) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_step(alpha = 0.5) +\n    labs(x = \"Time interval\", y = \"Accumulated evidence\", color = \"p\")\n\n\n\n\n\n\n\n\n\nWhat about evidence in cognitive tasks? Buridan’s ass relies on the same kind of sensory evidence as one needs to do, for example, psychophysical tasks like picking which stimulus is brighter, more leftward-oriented, etc. Evidence derived from memory can also be noisy—perhaps when retrieving an event, you sometimes recall the color of an object as blue and sometimes as green. When deciding between different gambles or products, we may also shift attention to different features of those options, leading us to judge them as better or worse depending on which features we attend to (Busemeyer & Townsend, 1993).\n\n\n3.1.4 Doing some code\nHaving now familiarized ourselves with how the random walk model represents a decision maker’s evidence and how it processes that evidence via accumulation, let’s see how we would write that model in code. Specifically, we will be writing code that simulates different possible random walks. The way we will do this is more of an intellectual exercise, since we will not be striving for efficiency (later on, we will use special-purposes R packages for that). Rather, the point here is to see how the conceptual aspects of a model can be implemented in code. We will add on to this code as we go.\nFor now, we know that we will have a line that looks something like the accumulation equation above:\n\n\nCode\nx &lt;- x + x_sample\n\n\nHere, x stands for the value of the accumulated evidence and x_sample stands for the current sample of evidence (which is either 1 or -1). The &lt;- evaluates the expression on the right and assigns it to the thing on the left, so the code above says “take the current value of x, add the new sample x_sample, and put it back as the new value of x”.\nWith the code above as the core of the model, we now need to do three things: first, specify how to get x_sample; second, obtain many such samples; third, keep a record of how the accumulated evidence changes over time.\n\n3.1.4.1 Sampling evidence\nTo get a value for x_sample, we will use R’s rbinom function, which generates a random sample from a binomial distribution. Specifically, the line rbinom(n = 1, size = 1, prob = 0.5) will generate a sample that equals 1 with probability 0.5, otherwise it equals zero. It is perhaps easiest to think of it in terms of a coin flip: The n parameter of the rbinom function says how many samples to draw, size says how many coins we flip at once, and prob is the probability that any single flip comes up heads. The number that rbinom gives is the number of heads per sample.\nFor Buridan’s ass, the sample of evidence favors each pile equally often, so prob = 0.5 makes sense. Note that because rbinom returns either a 0 or a 1, we need to do some math to turn the result into \\(+1\\) or \\(-1\\). This is shown below.\n\n\nCode\nx_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\nx &lt;- x + x_sample\n\n\n\n\n3.1.4.2 Obtaining many samples\nThere are a few ways we can write code that will obtain many samples. To anticipate what we will be doing later, we will use the while control structure. We can use it to specify a condition such that, so long as the condition is met, a block of code will continue to be executed in a loop.\nOur condition will depend on the current time. Remember that, in the random walk, each sample of evidence arrives at fixed intervals of time. We will therefore need to keep track of the current time as well as the accumulated evidence. Similar to how we updated the evidence, we will need to keep track of t, the current time. We will also need to specify dt, the duration of each interval, and t_max, the amount of time to keep accumulating evidence.\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n}\n\n\nNotice that we specified values for t_max and dt outside the while loop. We can specify initial values for x and t the same way:\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nx &lt;- 0\nt &lt;- 0\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n}\n\n\n\n\n3.1.4.3 Keeping a record\nThe chunk of code above will work just fine! But unfortunately it does not leave a record of accumulated evidence over time that we can then examine, like we did with the graph above. In the chunk below, we use a fun trick to keep a record of each value of x and t: We create two vectors x_record and t_record and use the c function to append the current values of x and t to these vectors:\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nx &lt;- 0\nt &lt;- 0\n\nx_record &lt;- x\nt_record &lt;- t\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n    x_record &lt;- c(x_record, x)\n    t_record &lt;- c(t_record, t)\n}\n\n\n\n\n3.1.4.4 Visualizing the record\nNow that we are keeping a record of evidence over time, let’s visualize it! The code below uses base R for that purpose, although the graph above uses ggplot2 which we will use again later. The type = \"s\" setting in the plot function at the end give the “step-like” plot.\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nx &lt;- 0\nt &lt;- 0\n\nx_record &lt;- x\nt_record &lt;- t\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n    x_record &lt;- c(x_record, x)\n    t_record &lt;- c(t_record, t)\n}\n\nplot(t_record, x_record, type = \"s\", xlab = \"Time\", ylab = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nTry copy-pasting the code above and running it yourself a few times to see what it looks like!\n\n\n3.1.4.5 Making a function\nIf we have a chunk of code that we want to re-run many times, we would do better to write a function that we can call instead of having to re-run the whole chunk. Writing a function also makes it easier to deal with parameters that can have different settings, like dt and t_max. We will also make the probability \\(p\\) a parameter too. Finally, the values for these three parameters in the function line are defaults.\n\n\nCode\nrw_sim &lt;- function(p = 0.5, dt = 0.05, t_max = 5) {\n    x &lt;- 0\n    t &lt;- 0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (t &lt; t_max) {\n        x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = p) - 1\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nNow we can call the function rw_sim with different settings to simulate different random walks. Note that, because the function returns t_record and x_record as different columns of a data.frame, we can easily use ggplot2 to plot the results, as in the examples below.\n\n\nCode\nsim_result1 &lt;- rw_sim(p = 0.5, dt = 0.05, t_max = 5)\n\nsim_result1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.5\")\n\n\n\n\n\n\n\n\n\nCode\nsim_result2 &lt;- rw_sim(p = 0.2, dt = 0.05, t_max = 5)\n\nsim_result2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.2\")\n\n\n\n\n\n\n\n\n\nCode\nsim_result3 &lt;- rw_sim(p = 0.8, dt = 0.05, t_max = 5)\n\nsim_result3 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.8\")\n\n\n\n\n\n\n\n\n\nGo ahead, try it out yourself with different values of p, dt, and/or t_max. It’s fun! And if you don’t think the step graphs are too interesting, just imagine that each of those steps is Simon the donkey trying to decide between his two piles of hay.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#making-a-decision",
    "href": "random_walk.html#making-a-decision",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "3.2 Making a decision",
    "text": "3.2 Making a decision\nSo far, we have built a simple model of evidence accumulation. In this model, samples of “evidence” arrive at regular intervals, with the sample supporting either one option (\\(+1\\)) or the other (\\(-1\\)) with probability \\(p\\), and the decision maker accumulates these samples by summation. The resulting accumulated evidence thus starts at zero and takes a “random walk” that can drift upward (if \\(p &gt; 0.5\\)), downward (if \\(p &lt; 0.5\\)), or in no particular direction (if \\(p = 0.5\\)).\n\n3.2.1 Response boundaries\nWhat we have not done is say how the decision maker uses this accumulated evidence to decide between their two options. According to the random walk model, the decision maker sets two values prior to accumulating evidence. These values are called thresholds, criteria, or boundaries (these terms are often used interchangeably). There is one positive boundary and one negative boundary. If and when the accumulated evidence crosses one of these boundaries, the decision maker selects the corresponding option.\nFor example, say that Buridan’s ass will pick the pile on the right if his accumulated evidence ever gets greater than \\(+5\\) and he will pick the pile on the left if his accumulated evidence ever gets less than \\(-5\\). We can visualize this situation by overlaying lines at those two boundaries on the “random walk” of accumulating evidence:\n\n\nCode\nburidan_sim1 &lt;- rw_sim(p = 0.5)\n\nburidan_sim1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nHere’s another one:\n\n\nCode\nburidan_sim2 &lt;- rw_sim(p = 0.5)\n\nburidan_sim2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nGo ahead and try it out yourself!\nThe point is that we can read from these graphs which option the donkey ends up picking by seeing which boundary gets crossed first. We can also see when the donkey makes his decision based on how long it took for that first boundary-crossing to occur. This is how the random walk model ultimately produces both a choice (which boundary was crossed first) and an RT (how long it took). It is also why the random walk saves Buridan’s ass: Even if the evidence in the long run does not favor either option, by chance the accumulated evidence will at some point cross one of the boundaries, enabling the donkey to make a decision.\n\n\n3.2.2 Response bias\nIn the examples above, Buridan’s ass set his response boundaries to be of equal distance from the initial evidence value of zero. Burdian’s ass might be more willing to go to the leftward pile than the rightward one—maybe it is more aesthetically appealing or the donkey has a limp that makes it easier for him to walk left than right. This would amount to a bias in favor of one option (going left) over the other (going right).\nWe can instantiate this bias in the random walk model via the donkey’s response boundaries. For example, the donkey may go to the left if the accumulated evidence ever gets less than \\(-4\\) but would only be willing to go to the right if the accumulated evidence ever gets greater than \\(+6\\). The following two graphs illustrate these biased response boundaries.\n\n\nCode\nburidan_bias_sim1 &lt;- rw_sim(p = 0.5)\n\nburidan_bias_sim1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"Response bias, simulation 1\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nburidan_bias_sim2 &lt;- rw_sim(p = 0.5)\n\nburidan_bias_sim2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"Response bias, simulation 2\")\n\n\n\n\n\n\n\n\n\nIntuitively, it seems reasonable to expect that, if one boundary is closer to the start than the other, that two things will happen: First, the option associated with the closer boundary will be picked more often (at least if the evidence against that option is not too strong). Second, the decision maker will tend to be faster to pick the option associated with the closer boundary. We will verify these intuitions later, but for now you can rest assured that these intuitions are correct.\n\n\n3.2.3 Revising our function\nNow that we have gotten acquainted with the notion of response boundaries and how they can be biased, let’s incorporate them into our random walk simulation function from earlier. This will involve two things: First, we will need to add two parameters to the function, one for each boundary. Second, we will need to change the condition in the while loop so that the random walk stops when it reaches a boundary. As a corrollary to this second step, we will keep the t_max condition but adjust the default value of t_max.\nThe revised function is shown below, with some additional explanation following:\n\n\nCode\nrw_sim &lt;- function(p = 0.5, b_upper = 5, b_lower = -5, dt = 0.05, t_max = Inf) {\n    x &lt;- 0\n    t &lt;- 0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = p) - 1\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nThe key changes we made to the rw_sim function are:\n\nAdding parameters b_upper and b_lower for the upper and lower response boundaries, respectively.\nChanging the default value of t_max to Inf for “infinity”. This means that, by default, reaching a boundary is the only way the random walk will stop. However, by leaving t_max as a parameter, it means that we can set it to some real number like 5 or 10 to force the random walk to stop eventually.\nChanging the condition in the while loop. Now the walk will continue so long as the evidence x is below the upper boundary (x &lt; b_upper) and above the lower boundary (x &gt; b_lower) and so long as the maximum time hasn’t been reached (t &lt; t_max). Note that the & is a “logical and” operator.\n\nHere are a few simulation runs—try it out yourself!\n\n\nCode\nboundary_sim_result1 &lt;- rw_sim(p = 0.5, b_upper = 5, b_lower = -5)\n\nboundary_sim_result1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.5, b_upper = 5, b_lower = -5\")\n\n\n\n\n\n\n\n\n\nCode\nboundary_sim_result2 &lt;- rw_sim(p = 0.5, b_upper = 6, b_lower = -4)\n\nboundary_sim_result2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.5, b_upper = 6, b_lower = -4\")\n\n\n\n\n\n\n\n\n\nCode\nboundary_sim_result3 &lt;- rw_sim(p = 0.7, b_upper = 6, b_lower = -4)\n\nboundary_sim_result3 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.7, b_upper = 6, b_lower = -4\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#residual-time",
    "href": "random_walk.html#residual-time",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "3.3 Residual time",
    "text": "3.3 Residual time\nWe are nearly done with our simulation model! We can model accumulating evidence and making a decision. The final ingredient arises from the fact that, while a decision maker might select one option at a particular time, we can only observe the behavioral consequences of that decision. Those behavioral consequences might be hitting a key, clicking a button, pressing a lever, or walking toward a pile of hay. Executing that behavior takes time in addition to the time needed to accumulate evidence and reach a response boundary. That additional time goes by many names, often “non-decision time” (NDT) or “encoding and responding” time (\\(T_{ER}\\)), but I prefer to simply call it residual time.\nFor now, we will adopt a simple assumption that this residual time is constant. Therefore, the observed response time will be the sum of the time needed for the random walk to reach a boundary plus the residual time associated with all the other processes that are involved in taking an action but which our model doesn’t explicitly enumerate.\nTo make this concrete, let’s introduce a parameter called t0 that will stand for residual time. While I cannot speak to what a plausible value of t0 would be for Buridan’s ass, in many cognitive tasks, it tends to be around 0.2 or 0.3 seconds, to account for the time needed to execute a simple motor action like hitting a button.\n\n\nCode\nrw_sim &lt;- function(p = 0.5, b_upper = 5, b_lower = -5, t0 = 0.2, dt = 0.05, t_max = Inf) {\n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = p) - 1\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nNote that the main change to our rw_sim function is that the initial value for the time t is no longer 0 but t0, i.e., the value of the residual time parameter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#simulating-many-trials",
    "href": "random_walk.html#simulating-many-trials",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "3.4 Simulating many trials",
    "text": "3.4 Simulating many trials\nOur rw_sim function can now simulate single realizations of a random walk decision process. As we have seen, though, each realization of this process is different because the samples of evidence are random. If we want to get a sense of the kind of behavior the model tends to produce, we need to simulate many realizations of the decision and examine the distribution of choices and RT’s produced by the model. This is the same reason why, in a typical cognitive task, we collect multiple trials from each participant in each condition. With a real participant, we are limited by the time and energy that a participant is willing to commit. With a model, we are still limited by time and energy, but they are our time and the computer’s energy. Nonetheless, it is worth keeping in mind that all the techniques below for visualizing choices and RT’s apply to observed data as well as they apply to simulated data.\n\n3.4.1 Running and saving many simulation results\nWe will need to write some code that repeatedly calls our rw_sim function a large number of times and saves the results so we can examine them later. What follows is not necessarily the most efficient way of accomplishing those goals, but it is conceptually transparent and introduces the for loop. The comments (following the # marks) explain what is going on with the line below.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- rw_sim(p = 0.5, b_upper = 5, b_lower = -5, t0 = 0.2, dt = 0.05)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Get a quick sense of what the results look like\nglimpse(sim_results)\n\n\nRows: 25,884\nColumns: 3\n$ t         &lt;dbl&gt; 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, …\n$ x         &lt;dbl&gt; 0, -1, -2, -3, -2, -3, -4, -3, -2, -3, -4, -3, -4, -3, -2, -…\n$ sim_index &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, …\n\n\n\n\n3.4.2 Visualizing the random walks\nWhat we are about to do may be a bit silly but helps build some intuitions about what is going on in the model. We are going to make a plot that overlays all 1000 simulated random walks on top of each other. The point is to get a sense of how much variability there is from one realization to the next.\n\n\nCode\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x, group = sim_index)) +\n    geom_step(alpha = 0.1) +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nOkay, maybe it is a bit silly after all. But it is possible to see that things “thin out” at longer times as more and more random walks end by hitting a boundary. If you check out the code that generates the plot, note how group = sim_index was used to make sure each individual simulation, indexed by sim_index, got its own step-line on the graph. Also note the use of alpha = 0.1 to make each line semi-transparent so they could be overlayed on one another.\nLet’s try a different approach to visualize the same thing, using a heatmap that indicates the relative frequency with which the accumulated evidence takes different values at different times:\n\n\nCode\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\")\n\n\n\n\n\n\n\n\n\nAgain, what is important to see above is that all the random walks start at the same time and evidence value (the yellow region) and then “fan out” over time.\n\n\n3.4.3 Joint distributions of choice and RT\nWhat we visualized in the previous section are the internal states of the model, that is, how the model represents the decision maker’s current balance of evidence between their two options. Remember, though, that the model is ultimately judged on its externally-observable behavior, since that is all we have to compare it against. We are finally going to visualize the choices and response times produced by the model. As we shall see, however, there are a few ways to do this!\n\n3.4.3.1 Extracting choices and RT’s\nFor each simulation, the RT is the final value of t, since that is the time (plus residual time) at which the first boundary was crossed. Meanwhile, the choice is whether the evidence x is positive or negative. The chunk of code below takes our simulation results and extracts the final choices and RT from each simulation.\n\n\nCode\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\nglimpse(choice_rt)\n\n\nRows: 1,000\nColumns: 3\n$ sim_index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ choice    &lt;fct&gt; lower, lower, lower, upper, upper, lower, upper, lower, uppe…\n$ rt        &lt;dbl&gt; 1.05, 2.35, 0.95, 0.75, 0.55, 2.45, 1.15, 0.45, 1.35, 3.25, …\n\n\n\n\n3.4.3.2 Joint frequency plot\nThe code below plots the frequency with which each choice (upper or lower) is made at different times. This kind of plot is not terribly common, but is a quick way to get a sense of both how often each choice is made as well as the shape of the distributions of RT’s.\n\n\nCode\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_freqpoly(binwidth = 0.2) +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\")\n\n\n\n\n\n\n\n\n\n\n\n3.4.3.3 Conditional RT density\nThe code below plots the conditional density of the RT’s for each choice. This kind of plot is much more common, but doesn’t convey any information about the relative frequency with which different choices are made. Nonetheless, it illustrates how the random walk produces distributions of RT’s with a pronounced right skew, similar to RT distributions that are actually observed in choice tasks. Note that the conditional RT distributions for each choice are pretty similar to one another too.\n\n\nCode\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\")\n\n\n\n\n\n\n\n\n\n\n\n3.4.3.4 Quantile-probability plots\nIn the choice-RT modeling world, it is common to make “quantile-probability plots”, sometimes abbreviated to QP plots. These plots can be a bit confusing at first, but are useful because they convey information about choice proportions and RT distributions in a single graph.\nThe horizontal axis of a QP plot corresponds to the probability of having made a particular choice. In this case, that is the proportion of simulations that resulted in each choice. We can get that information in numerical form from our choice_rt data frame:\n\n\nCode\nchoice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\n\n# A tibble: 2 × 3\n  choice     n p_resp\n  &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 upper    507  0.507\n2 lower    493  0.493\n\n\nThe vertical axis of a QP plot corresponds to different quantiles of the conditional RT distributions for each choice. Typically, those quantiles are the RT’s at the 10th, 30th, 50th, 70th, and 90th percentiles of the distribution. The reason for all of these quantiles is that they convey information about different aspects of the distribution: The 50th percentile, otherwise known as the median, conveys the central tendency. The 30th and 70th percentiles indicate where the “bulk” of the RT’s tend to fall. Finally, the 10th and 90th percentiles convey information about the lower and upper tails of the distribution, respectively. We can obtain those quantiles numerically like so:\n\n\nCode\nchoice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\n\n# A tibble: 10 × 2\n   choice  rt_q\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 upper   0.55\n 2 upper   0.75\n 3 upper   1.05\n 4 upper   1.55\n 5 upper   2.65\n 6 lower   0.55\n 7 lower   0.85\n 8 lower   1.25\n 9 lower   1.75\n10 lower   2.75\n\n\nTo make a QP plot, we need to “join” together the response proportions and RT quantiles into the same data frame:\n\n\nCode\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q)\n\n\nJoining with `by = join_by(choice)`\n\n\n# A tibble: 10 × 4\n   choice     n p_resp  rt_q\n   &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 upper    507  0.507  0.55\n 2 upper    507  0.507  0.75\n 3 upper    507  0.507  1.05\n 4 upper    507  0.507  1.55\n 5 upper    507  0.507  2.65\n 6 lower    493  0.493  0.55\n 7 lower    493  0.493  0.85\n 8 lower    493  0.493  1.25\n 9 lower    493  0.493  1.75\n10 lower    493  0.493  2.75\n\n\nThat joined data frame can then be used as the basis for our QP plot:\n\n\nCode\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#putting-it-all-together",
    "href": "random_walk.html#putting-it-all-together",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "3.5 Putting it all together",
    "text": "3.5 Putting it all together\nWe have used R to build a random walk model of decision making, implemented via a function called rw_sim, that accumulates samples of evidence until the accumulated evidence reaches either an upper or lower boundary. This model depends on several parameters, of which the most theoretically important are:\n\np: The probability that any given sample of evidence favors the option associated with the upper response boundary.\nb_upper: The upper response boundary.\nb_lower: The lower response boundary.\nt0: Residual time.\n\nWe also saw different ways that we can visualize both the internal states and external behavior of the model. It may be useful at this point to put together everything we have done so far into a single chunk of code. This will make your own explorations of this model easier.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- rw_sim(p = 0.5, b_upper = 5, b_lower = -5, t0 = 0.2, dt = 0.05)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#exercises",
    "href": "random_walk.html#exercises",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "3.6 Exercises",
    "text": "3.6 Exercises\n\nSet the p parameter to something other than 0.5, so that the evidence tends to favor one option over the other. Do one set of simulations in which the response boundaries are equidistant from the starting value of 0 (you may need to play around to find values that you like). Do another set of simulations in which you keep the boundaries equidistant but make them closer to the starting point. What is the effect on the model’s choices and RT’s of having boundaries that are closer to the starting point?\nRun one set of simulations with the p parameter to 0.6 and the response boundaries equidistant from the starting point. Run another set of simulations keeping the response boundaries the same but increasing the p parameter to 0.8. What is the effect of increasing the p parameter on the RT distributions for making the “upper” choice? What is the effect of increasing the p parameter on the RT distributions for making the “lower” choice?\nImagine that, instead of each sample of evidence equalling either \\(+1\\) or \\(-1\\), the evidence could also equal \\(0\\). Write code to simulate this model and use your simulations to see how this model might differ from the random walk model we developed in this chapter.\n\nYou will need to introduce a new parameter to the model that represents the probability of getting a sample that equals zero. What ways can you think of to implement this aspect of the model? Which method did you pick and why?\nHow does the shape of the predicted RT distributions differ, if at all, from that predicted by the original random walk model? (Hint: you may want to explore settings in which there is zero probability of taking a step either up or down. It may also help to visualize the random walks themselves too.)\nWhat cognitive tasks might be better modeled by allowing for evidence to have a value of zero?\n\nTry implementing a model in which the residual time can vary randomly according to some distribution. Since residual time must be non-negative, you might consider distributions like the Gamma distribution or a uniform distribution between two positive values.\n\nHow did you implement random residual times?\nHow does random residual time affect the shape of the predicted RT distributions?\nWhat psychological factors might contribute to variability in residual time?\n\n\n\n\n\n\nBusemeyer, J. R., & Townsend, J. T. (1993). Decision field theory: A dynamic–cognitive approach to decision making in an uncertain environment. Psychological Review, 100(3), 432–459.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html",
    "href": "diffusion_sim.html",
    "title": "4  From random walk to diffusion",
    "section": "",
    "text": "4.1 Discrete to continuous evidence\nIn the previous chapter, we built a random walk model of how someone might decide between two options. In this chapter, we turn the random walk model into the more widely applied diffusion model, introduced to psychology by Ratcliff (1978). As we shall see, conceptually, the transition from a random walk to a diffusion model is not very large.\nLike most models of choice and RT, both random walk and diffusion models are premised on the idea that making a decision requires accumulating samples of “evidence” until the accumulated evidence reaches a response boundary. The “evidence” in these models is deliberately abstract because these models are meant to be applied in a variety of situations. The important thing is that “evidence” can be represented in these models as a number, where a sample of evidence supporting one option takes a positive value while a sample supporting the other option takes a negative value. A diffusion model differs from a random walk model in two aspects regarding the nature of the evidence that is accumulated:\nIn other words, the random walk model treats evidence as taking discrete values that are sampled at discrete intervals, whereas a diffusion model treats evidence as taking continuous values that are sampled continuously in time.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#discrete-to-continuous-evidence",
    "href": "diffusion_sim.html#discrete-to-continuous-evidence",
    "title": "4  From random walk to diffusion",
    "section": "",
    "text": "Samples of evidence take continuous values, rather than discrete values.\nSamples of evidence arrive continually, rather than at regular intervals.\n\n\n\n4.1.1 Evidence sampled from a normal distribution\nIn the random walk model, the magnitude of each sample of evidence was always equal to one. Each sample was either \\(+1\\) or \\(-1\\). In a diffusion model, evidence can take any real value, such that its magnitude is now important. Conceptually, this has some intuitive appeal. Some samples of evidence strongly favor one option, some samples only weakly support one option, and some are equivocal.\nIn a diffusion model, samples of evidence are specifically assumed to come from a normal distribution. The standard deviation of this distribution is typically fixed to some value like 0.1 or 1. Here, we will fix it to the value of 1. The reason for fixing this value is that “evidence” is abstract and therefore has no natural scale. We could multiply or divide all the evidence samples by a constant amount without changing their underlying meaning.\nThe mean of the evidence distribution represents how strongly the evidence tends to favor one option over the other, similar in meaning to the \\(p\\) parameter in the random walk model. The mean of the evidence distribution in a diffusion model is termed the drift rate, as it reflects the tendency for accumulated evidence to “drift” either upward or downward over time. As illustrated in the graph below, the mean of the evidence distribution governs the degree to which samples support one option versus the other.\n\n\nCode\nexpand_grid(v = c(-2, -1, 0, 1, 2), x = seq(-4, 4, length.out = 201)) %&gt;%\n    mutate(d = dnorm(x, mean = v, sd = 1)) %&gt;%\n    ggplot(aes(x = x, y = d, color = v, group = v)) +\n    geom_vline(xintercept = 0, linetype = \"dashed\") +\n    geom_line() +\n    scale_color_gradient2(mid = \"#444444\", midpoint = 0) +\n    labs(x = \"Value of evidence sample\", y = \"Relative frequency\", color = \"Mean of evidence\\ndistribution\")\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Evidence sampled continuously in time\nHere we come to a bit of a subtle issue: In the random walk, evidence arrived in discrete units at regular intervals, but the duration of the interval was not related to the magnitude of the evidence. In a diffusion model, we assume that evidence arrives continuously in time. One way to think about this—indeed, the way that we will simulate this—is that evidence is sampled in many very short intervals of time, each of which has duration \\(\\Delta t\\). When \\(\\Delta t\\) is small enough, those many little intervals will look like one continuous span of time. This principle is illustrated in the graph below.\n\n\nCode\ndiffusion_sim &lt;- expand_grid(dt = c(0.1, 0.01, 0.001)) %&gt;%\n    group_by(dt) %&gt;%\n    reframe(t = seq(0, 3, by = dt)) %&gt;%\n    ungroup() %&gt;%\n    mutate(x_sample = rnorm(n = n(), mean = 0, sd = 1 * sqrt(dt))) %&gt;%\n    group_by(dt) %&gt;%\n    mutate(x = cumsum(x_sample))\n\nscaled_plot &lt;- diffusion_sim %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    facet_wrap(\"dt\", labeller = label_bquote(Delta * t == .(dt))) +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = expression(\"Evidence scaled by \" * Delta * t))\n\nunscaled_plot &lt;- diffusion_sim %&gt;%\n    ggplot(aes(x = t, y = x / sqrt(dt))) +\n    geom_step() +\n    facet_wrap(\"dt\", labeller = label_bquote(Delta * t == .(dt))) +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"Evidence not scaled\")\n\nscaled_plot / unscaled_plot\n\n\n\n\n\n\n\n\n\nThe top set of graphs above show how, when \\(\\Delta t\\) is sufficiently small, the trajectory of accumulated evidence looks essentially continuous—you can no longer see the “jumps” from one interval to the next.\nThe bottom set of graphs illustrate the subtlety I mentioned earlier. If we divide time into many small intervals but leave the mean and standard deviation of the evidence distribution the same, then we are essentially getting many more samples of evidence. As a result, accumulated evidence has a much larger scale than it would have if we had picked a smaller \\(\\Delta t\\). From a theoretical standpoint, this doesn’t make sense—the rate at which evidence accumulates for a decision should not be affected by the modeler’s arbitrary choice of \\(\\Delta t\\).\nSo what we do is scale the evidence samples by \\(\\Delta t\\). That’s what was done in the top set of graphs, but not the bottom set. The idea is that if you have very small time intervals, you shouldn’t be able to get as large of a sample of evidence. Again, this makes theoretical sense, if evidence is something that takes time to accumulate.\nSpecifically, a diffusion model assumes that each sample of evidence is drawn from a normal distribution with a mean of \\(v \\times \\Delta t\\), where \\(v\\) is the drift rate parameter, and a standard deviation of \\(\\sqrt{\\Delta t}\\). Why \\(\\sqrt{\\Delta t}\\) instead of just \\(\\Delta t\\)? Because it is the mean and variance that need to be scaled by \\(\\Delta t\\).\n\n\n4.1.3 A new simulation function\nLet’s take our rw_sim function from the previous chapter and turn it into a diffusion model. To do this, we make two modifications: First, we swap out the p parameter representing the probability of getting a positive sample for a parameter called v which is the drift rate. Second, instead of getting each evidence sample x_sample from a binomial distribution, we will get it from a normal distribution using R’s rnorm function. These changes are illustrated below.\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, b_upper = 1, b_lower = -1, t0 = 0.2, dt = 0.01, t_max = Inf) {\n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = v * dt, sd = sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nIn the code above, I also took the liberty of adjusting the default values of b_upper, b_lower, and dt so that the simulated choices and RT’s would look a bit more like those observed in cognitive tasks, but of course you may feel free to adjust those yourself as you like.\n\n\n4.1.4 Putting it all together—again\nAt the end of the last chapter, I included a chunk of code that simulated a random walk and produced some visualizations to help us understand both its internal states and its overt behavior (choices and RT). By swapping out rw_sim with the appropriately adjusted diffusion_sim line, we can apply the same chunk of code to the diffusion model! In the chunk below, I picked some arbitrary but reasonable values for the parameters.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, b_upper = 1, b_lower = -1)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\nYou may or may not be surprised to see that the RT distributions produced by the diffusion model closely resemble those produced by the random walk! The diffusion model also demonstrates an interesting feature of a random walk, namely, that the conditional RT distribution depends on the boundaries but not on the drift rate. In the example above, I set \\(v = 0.5\\), such that evidence would tend to favor the positive option. Even though the model ends up choosing that option more often, it does not do so any faster or slower than it chooses the negative option. This is something we will return to at the end of this chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#response-caution-and-response-bias",
    "href": "diffusion_sim.html#response-caution-and-response-bias",
    "title": "4  From random walk to diffusion",
    "section": "4.2 Response caution and response bias",
    "text": "4.2 Response caution and response bias\nBefore confronting the issue of invariant RT distributions, it behooves us to consider a different way of specifying the response boundaries in our model. So far, we have specified those boundaries directly. We can speak of response caution in terms of how far those boundaries are from the starting point and response bias in terms of whether the boundaries are equidistant from the starting point.\nSpecifically, we could define a term \\(A\\) that is the total distance between the starting point (zero) and the two boundaries. If \\(B_{\\text{Upper}}\\) and \\(B_{\\text{Lower}}\\) are the upper and lower boundaries, respectively, then \\[\nA = B_{\\text{Upper}} - B_{\\text{Lower}}\n\\] In other words, \\(A\\) is how far apart the two boundaries are, called boundary separation. The term \\(A\\) can be seen to operationalize the construct of response caution in that a decision maker who wants to wait to accumulate evidence would put their response boundaries far apart.\nWe can also operationalize the construct of response bias by defining a term \\(w\\). This term will be a number between 0 and 1 that represents the degree to which response boundaries favor one choice over the other. Specifically, let \\[\nw = \\frac{-B_{\\text{Lower}}}{B_{\\text{Upper}} - B_{\\text{Lower}}}\n\\] As shown in the graph below, \\(w = 0.5\\) when the boundaries are equidistant from zero, \\(w &lt; 0.5\\) when the boundaries are biased in favor of the negative option, and \\(w &gt; 0.5\\) when the boundaries are biased in favor of the positive option.\n\n\nCode\nexpand_grid(b_upper = seq(1, 5), b_lower = seq(-1, -5)) %&gt;%\n    mutate(A = b_upper - b_lower) %&gt;%\n    mutate(w = -b_lower / A) %&gt;%\n    pivot_longer(c(A, w), names_to = \"par\", values_to = \"val\") %&gt;%\n    mutate(par = factor(par, levels = c(\"A\", \"w\"), labels = c(\"Response caution (A)\", \"Response bias (w)\"))) %&gt;%\n    ggplot(aes(x = b_upper, y = val, color = b_lower, group = b_lower)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(\"par\", scales = \"free_y\", strip.position = \"left\") +\n    labs(x = expression(B[\"Upper\"]), y = NULL, color = expression(B[\"Lower\"])) +\n    theme(strip.placement = \"outside\", strip.background = element_blank())\n\n\n\n\n\n\n\n\n\nHaving defined \\(A\\) and \\(w\\) as ways of operationalizing response caution and response bias, respectively, why not treat these values as parameters instead of the boundaries themselves? The value in doing so is that we can then specify these constructs directly, rather than having to work backwards from the boundaries. Specifically, if we pick values of \\(A\\) and \\(w\\) we can immediately compute what the upper and lower boundaries should be: \\[\\begin{align}\nB_{\\text{Upper}} & = w A \\\\\nB_{\\text{Lower}} & = -\\left(1 - w \\right) A \\\\\n\\end{align}\\]\nAnd we can adjust our diffusion_sim code accordingly to have a and w as parameters instead of b_upper and b_lower, which now get calculated in the function itself:\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf) {\n    b_upper &lt;- (1 - w) * a\n    b_lower &lt;- -w * a\n    \n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#trial-by-trial-variability",
    "href": "diffusion_sim.html#trial-by-trial-variability",
    "title": "4  From random walk to diffusion",
    "section": "4.3 Trial-by-trial variability",
    "text": "4.3 Trial-by-trial variability\nRecall that both the random walk and the diffusion model have the following property: The response times they produce depend on the distance between the starting point and the response boundary, not on the drift rate \\(v\\) or the step probability \\(p\\). To see why this might be problematic from a psychological perspective, imagine that the upper boundary corresponds to making a correct response while the lower boundary corresponds to making an error. Assume that \\(v &gt; 0\\), such that the evidence tends to favor making a correct response. The fact that response times do not depend on drift rates means that the model predicts that correct and error responses will be made in the same amount of time. To be more precise, the distribution of RT’s conditional on accuracy are the same.\nOften, errors are either faster or slower than correct responses. For example, it may be that errors occur more often when the decision maker happens to get poor evidence. In that case, we might expect errors to be slow because they result from the decision maker deliberating longer in the face of this poor evidence. On the other hand, maybe a decision maker tends to be correct when they take their time, but will sometimes “jump the gun” and pick the wrong option, in which case we would expect errors to be faster than correct responses.\nThe critical factor that Ratcliff (1978) introduced to the diffusion model that has made it into such a useful tool is that the drift rate is not the same on every trial, but varies randomly from trial to trial. On some trials, you happen to get a drift rate in the high positive tail of the distribution of drift rates, in which case you would probably make a fast correct response. On other trials, you happen to get a drift rate that is close to zero or even falls below zero by chance, in which case you would be more likely to make an error and would tend to do so more slowly. Thus, trial-by-trial variability in drift rates accounts for slow errors.\nWhat about fast errors? Ratcliff & Rouder (1998) showed that these can result if your response boundaries are not always fixed, but can also vary randomly from trial to trial. Sometimes, they happen to be very close to the starting point such that it takes very little evidence to commit to a response. Such rapid responses would be more likely to be errors, since they don’t give much time to accumulate evidence. Thus, trial-by-trial variability in boundaries (or, equivalently, in starting point) accounts for fast errors.\nThere is a final thing that can vary from trial to trial, and that is residual time. After all, if the time needed to accumulate evidence can vary between trials, so can the time needed to accomplish all the other processes involved in any given decision task. Trial-by-trial variability in residual time does not, of course, affect the probability of choosing either option, but it does affect the form of the RT distributions.\n\n4.3.1 Adding variability to our simulation code\nTo model each of these kinds of trial-by-trial variability, we need to decide how each of the values above (drift rate, boundaries, and residual time) can vary. This will also inform us as to what new parameters we will need to add to our model to specify that variability. In what follows, we will adopt common assumptions in the literature that are also implemented in the model-fitting functions we will use in later chapters. Check out the exercises (or explore on your own) to consider other forms of trial-by-trial variability!\n\n4.3.1.1 Trial-by-trial variability in drift rates\nOur model already has a parameter called \\(v\\) that stands for the “drift rate”. Let us instead treat \\(v\\) as the mean of a normal distribution of drift rates, which has standard deviation \\(s_v\\). If \\(s_v = 0\\), then we have our original diffusion model with the same drift rate on every trial. On the other hand, if \\(s_v &gt; 0\\), then the drift rate on any given trial will sometimes be greater or less than \\(v\\), even if the average drift rate across all trials is \\(v\\).\nTo implement this in our simulation code, we need to\n\nAdd a new parameter sv.\nAdd a line that randomly samples the drift rate (called trial_v) from a normal distribution with mean v and standard deviation sv.\nReplace v when drawing samples of evidence with trial_v.\n\nThese changes are reflected in the following adjusted code:\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf, sv = 0) {\n    trial_v &lt;- rnorm(n = 1, mean = v, sd = sv)\n    \n    b_upper &lt;- (1 - w) * a\n    b_lower &lt;- -w * a\n    \n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = trial_v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nThe chunk of code below has the same settings as that shown above, only now sv = 1. As you can see, responses on the lower boundary have a different RT distribution, which tends to be slower, than responses on the upper boundary. (Note, too, that I am using our revised code that uses a and w to define the response boundaries.)\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0.5)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\n\n\n4.3.1.2 Trial-by-trial variability in boundaries/starting point\nThere are a number of ways that we could introduce variability in the starting point and/or boundaries. To be consistent with the model-fitting we will do later, we will assume that the bias parameter \\(w\\) is not fixed, but is sampled from a uniform distribution that goes from \\(w - \\frac{s_w}{2}\\) to \\(w + \\frac{s_w}{2}\\). Thus, the average bias is still \\(w\\) but has a range defined by parameter \\(s_w\\). As above, we need to add this new parameter and randomly sample a trial_w value at the top of our code. Note that the line that samples trial_w does some checking using the min and max functions to make sure that \\(w\\) never falls below 0 or greater than 1.\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf, sv = 0, sw = 0) {\n    trial_v &lt;- rnorm(n = 1, mean = v, sd = sv)\n    trial_w &lt;- runif(n = 1, min = max(0, w - 0.5 * sw), max = min(1, w + 0.5 * sw))\n    \n    b_upper &lt;- (1 - trial_w) * a\n    b_lower &lt;- -trial_w * a\n    \n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = trial_v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nThe simulations below set \\(s_v = 0\\) and \\(s_w = 0.9\\), while \\(v = 0.5\\). In the simulations below, when the model picks the “incorrect” option associated with the lower boundary, it is predicted to do so faster than when it responds by choosing the “correct” option associated with the upper boundary.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0, sw = 0.9)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\n\n\n4.3.1.3 Trial-by-trial variability in residual time\nOur final amendment to our diffusion simulation code involves adding variability to the residual time. Again, there are many ways we could do this, but we will adopt the same conventional approach used in our model-fitting later: We will assume that the residual time on any given trial is sampled from a uniform distribution that ranges from \\(t_0\\) to \\(t_0 + s_{t_0}\\). The code below adds the new st0 parameter and samples a residual time trial_t0 from a uniform distribution at the beginning of the simulation:\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf, sv = 0, sw = 0, st0 = 0) {\n    trial_v &lt;- rnorm(n = 1, mean = v, sd = sv)\n    trial_w &lt;- runif(n = 1, min = max(0, w - 0.5 * sw), max = min(1, w + 0.5 * sw))\n    trial_t0 &lt;- runif(n = 1, min = t0, max = t0 + st0)\n    \n    b_upper &lt;- (1 - trial_w) * a\n    b_lower &lt;- -trial_w * a\n    \n    x &lt;- 0\n    t &lt;- trial_t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = trial_v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nThe simulations below again assume that \\(v = 0.5\\) and set \\(s_v = s_w = 0\\) while \\(s_t = 0.5\\). Note that the resulting RT distributions end up having a longer early tail, reflecting greater variability in the fastest RT’s due to variability in residual time.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0, sw = 0, st0 = 0.6)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 Putting it all together (finally!)\nFinally, for completeness, let’s collect our code to run a set of simulations that allows for all three kinds of variability. This code is not executed here, but is included so it can serve as the basis for your own simulations and explorations.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0.5, sw = 0.2, st0 = 0.4)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#shiny-app",
    "href": "diffusion_sim.html#shiny-app",
    "title": "4  From random walk to diffusion",
    "section": "4.4 Shiny App",
    "text": "4.4 Shiny App",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#exercises",
    "href": "diffusion_sim.html#exercises",
    "title": "4  From random walk to diffusion",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\n\nUnder what circumstances do you think it is more likely for errors to be faster than correct responses? What about circumstances in which errors are more likely to be slower than correct responses? What do you think about the psychological implications of how the diffusion model produces either fast or slow errors?\nWrite a new diffusion simulation that uses a different distribution of drift rates from trial to trial—you might try distributions that are skewed (like an ExGaussian) or have heavy tails (like the T distribution with few degrees of freedom).\n\nDescribe the distribution you picked and whether it corresponds to a theory or hypothesis about how evidence may vary from trial-to-trial in a particular cognitive task.\nDescribe any differences you find between the model you wrote and the model in the chapter that assumes a normal distribution of drift rates across trials.\n\n\n\n\n\n\nRatcliff, R. (1978). A theory of memory retrieval. Psychological Review, 85(2), 59–108.\n\n\nRatcliff, R., & Rouder, J. N. (1998). Modeling response times for two-choice decisions. Psychological Science, 9(5), 347–356.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ashby, F. G. (1983). A biased random walk model for two choice reaction\ntimes. Journal of Mathematical Psychology, 27,\n277–297.\n\n\nAshby, F. G. (2000). A stochastic version of general recognition theory.\nJournal of Mathematical Psychology, 44, 310–329.\n\n\nBaum, C. W., & Veeravalli, V. V. (1994). A sequential procedure for\nmultihypothesis testing. IEEE Transactions on\nInformation Theory, 40(6), 1994–2007.\n\n\nBogacz, R., Brown, E., Moehlis, J., Holmes, P., & Cohen, J. D.\n(2006). The physics of optimal decision making: A formal analysis of\nmodels of performance in two-alternative forced-choice tasks.\nPsychological Review, 113(4), 700–765.\n\n\nBogacz, R., & Gurney, K. (2007). The basal ganglia and cortex\nimplement optimal decision making between alternative actions.\nNeural Computation, 19, 442–477.\n\n\nBogacz, R., Usher, M., Zhang, J., & McClelland, J. L. (2007).\nExtending a biologically inspired model of choice: Multi-alternatives,\nnonlinearity and value-based multidimensional choice. Philosophical\nTransactions of the Royal Society B, 362, 1655–1670.\n\n\nBroadbent, D. E. (1957). A mechanical model for human attention and\nimmediate memory. Psychological Review, 64(3),\n205–215.\n\n\nBrown, S., & Heathcote, A. (2005). A ballistic model of choice\nresponse time. Psychological Review, 112(1), 117–128.\n\n\nBrown, S., & Heathcote, A. (2008). The simplest complete model of\nchoice response time: Linear ballistic accumulation. Cognitive\nPsychology, 57, 153–178.\n\n\nBusemeyer, J. R., & Townsend, J. T. (1993). Decision field theory: A\ndynamic–cognitive approach to decision making in an uncertain\nenvironment. Psychological Review, 100(3), 432–459.\n\n\nCox, G. E., & Shiffrin, R. M. (In press). Computational models of\nevent memory. In M. J. Kahana & A. Wagner (Eds.), Oxford\nhandbook of human memory. Oxford University Press.\n\n\nDonkin, C., Brown, S., Heathcote, A., & Wagenmakers, E.-J. (2011).\nDiffusion versus linear ballistic accumulation: Different models but the\nsame conclusions about psychological processes? Psychonomic Bulletin\n& Review, 55, 140–151.\n\n\nEdwards, W. (1965). Optimal strategies for seeking information: Models\nfor statistics, choice reaction times, and human information processing.\nJournal of Mathematical Psychology, 2, 312–329.\n\n\nJones, M., Mozer, M., & Kinoshita, S. (2009). Optimal response\ninitiation: Why recent experience matters. In D. Koller, D. Schuurmans,\nY. Bengio, & L. Bottou (Eds.), Advances in neural information\nprocessing systems (Vol. 21, pp. 785–792).\n\n\nLaBerge, D. (1962). A recruitment theory of simple behavior.\nPsychometrika, 27(4), 375–396.\n\n\nLink, S. W. (1975). The relative judgment theory of two choice response\ntime. Journal of Mathematical Psychology, 12, 114–135.\n\n\nLink, S. W., & Heath, R. A. (1975). A sequential theory of\npsychological discrimination. Psychometrika, 40,\n77–105.\n\n\nMacmillan, N. A., & Creelman, C. D. (2005). Detection theory: A\nuser’s guide (2nd ed.). Erlbaum.\n\n\nNeisser, U. (1967). Cognitive psychology.\nAppleton-Century-Crofts.\n\n\nNeyman, J., & Pearson, E. S. (1933). On the problem of the most\nefficient tests of statistical hypotheses. Philosophical\nTransactions of the Royal Society of London. Series A, Containing Papers\nof a Mathematical or Physical Character, 231, 289–337.\n\n\nPike, R. (1973). Response latency models for signal detection.\nPsychological Review, 80(1), 53–68.\n\n\nRatcliff, R. (1978). A theory of memory retrieval. Psychological\nReview, 85(2), 59–108.\n\n\nRatcliff, R., & Rouder, J. N. (1998). Modeling response times for\ntwo-choice decisions. Psychological Science, 9(5),\n347–356.\n\n\nRatcliff, R., & Smith, P. L. (2004). A comparison of sequential\nsampling models for two-choice reaction time. Psychological\nReview, 111(2), 333–367.\n\n\nSingmann, H., Kellen, D., Cox, G. E., Chandramouli, S. H., Davis-Stober,\nC. P., Dunn, J. C., Gronau, Q. F., Kalish, M. L., McMullin, S. D.,\nNavarro, D. J., & Shiffrin, R. M. (2022). Statistics in the service\nof science: Don’t let the tail wag the dog. Computational Brain\n& Behavior.\n\n\nSmith, P. L. (2000). Stochastic dynamic models of response time and\naccuracy: A foundational primer. Journal of Mathematical\nPsychology, 44(3), 408–463.\n\n\nSmith, P. L., & Van Zandt, T. (2000). Time-dependent\nPoisson counter models of response latency in simple\njudgment. British Journal of Mathematical and Statistical\nPsychology, 53, 293–315.\n\n\nSmith, P. L., & Vickers, D. (1988). The accumulator model of\ntwo-choice decision. Journal of Mathematical Psychology,\n32, 135–168.\n\n\nStone, M. (1960). Models for choice-reaction time.\nPsychometrika, 25, 251–260.\n\n\nSwensson, R. G., & Green, D. M. (1977). On the relations between\nrandom walk models for two-choice response times. Journal of\nMathematical Psychology, 15, 282–291.\n\n\nTeodorescu, A. R., & Usher, M. (2013). Disentangling decision\nmodels: From independence to competition. Psychological Review,\n120(1), 1–38.\n\n\nTownsend, J. T., & Ashby, F. G. (1983). Stochastic modeling of\nelementary psychological processes. Cambridge University Press.\n\n\nUsher, M., & McClelland, J. L. (2001). The time course of perceptual\nchoice: The leaky, competing accumulator model. Psychological\nReview, 108(3), 550–592.\n\n\nUsher, M., & McClelland, J. L. (2004). Loss aversion and inhibition\nin dynamical models of multialternative choice. Psychological\nReview, 111(3), 757–769.\n\n\nVickers, D. (1970). Evidence for an accumulator model of psychophysical\ndiscrimination. Ergonomics, 13(1), 37–58.\n\n\nWald, A. (1945). Sequential tests of statistical hypotheses. The\nAnnals of Mathematical Statistics, 16(2), 117–186.\n\n\nZhang, J., & Chang, M. (in review). A Bayesian\nrandom-walk model of choice reaction times under prior knowledge of\ntarget onset uncertainty. Proceedings of the National Academy of\nSciences.",
    "crumbs": [
      "References"
    ]
  }
]